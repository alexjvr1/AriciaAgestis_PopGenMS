# Adaptation at new sites

1) Adaptation to different host plants?

2) Selection for dispersing individuals? 



## Outlier analyses

### PCAdapt

Initial PCAdapt steps completed in [3.GeneticDiversity&Structure.md](https://github.com/alexjvr1/AriciaAgestis_PopGenMS/blob/master/3.GeneticDiversity%26Structure.md)


Vignette: https://cran.r-project.org/web/packages/pcadapt/vignettes/pcadapt.html

Useful info about interpreting p-value distributions: http://varianceexplained.org/statistics/interpreting-pvalue-histogram/

I did this in R following the vignette quite closely:

I chose K=2 following the scree plot from PCAdapt

```
AA261.bed <- read.pcadapt("plink.bed", type="bed")
x.maf0.05 <- pcadapt(AA261.bed, K=2, min.maf=0.05)   ##calculate z-statistics and transformed values for chi-squared distribution
x.maf0.1 <- pcadapt(AA261.bed, K=2, min.maf=0.01) #calculate for maf 0.1

pdf(file="pcadapt.pvalues_AA261.pdf")
par(mfrow=c(2,1))
hist(x.maf0.05$pvalues,xlab="p-values maf0.05",main=NULL,breaks=50)
hist(x.maf0.1$pvalues,xlab="p-values maf0.01",main=NULL,breaks=50)
dev.off()

```


![alt_txt][maf]

[maf]:https://user-images.githubusercontent.com/12142475/54209447-b2343700-44d5-11e9-82ed-51999125f95c.png



I'll use a MAF cut-off of 1% as this yields more candidate loci, and the p-value distribution is fairly normal. 


use qvalue in R to specify FDR

```
library(qvalue)
alpha <- 0.05
qval <- qvalue(x.maf0.05$pvalues)$qvalues
outliers <- which(qval<alpha)
outliers
snp_pc <- get.pc(x.maf0.05,outliers) ##see PCs associated with the outliers
```

The outliers identified are mostly associated with principal component 2. I.e. they explain the divergence of HOD from the rest of the populations. 


Write qval for both analyses to file. These will be ordered later for comparison between all analyses
```
qval <- qvalue(x.maf0.05$pvalues)$qvalues
write.table(qval, "PCAdapt.qval.maf0.05_AA216", quote=F, row.names=F, col.names=F)

qval <- qvalue(x.maf0.1$pvalues)$qvalues
write.table(qval, "PCAdapt.qval.maf0.01_AA216", quote=F, row.names=F, col.names=F)
```



### Bayescan

Convert vcf to bayescan using pgdspider. 

Copy file to bluecrystal to run analysis. 

```
pwd
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/Bayescan

scp AA216.bayescan aj18951@bluecrystalp3.acrc.bris.ac.uk:/newhome/aj18951/1a_Aricia_agestis_PopGenomics/Bayescan/
```


I've created a submission script (bayescan.sh) on bluecrystal. To be run from the directory where the input file is located.
```
#!/bin/bash
#PBS -N bayescan1
#PBS -l nodes=1:ppn=1 #1nodes, 1 processor per node
#PBS -l mem=16gb #RAM
#PBS -l walltime=100:00:00 ##20 hours wall time.  
#PBS -j oe  #concatenates error and output files (with prefix job1)
##PBS -t 1-10

#run job in working directory
cd $PBS_O_WORKDIR  #run job in working directory
#cd 1a_Aricia_agestis_PopGenomics/Bayescan

# set a local temporary folder.  Useful if you create a lot of temp files and do not want to receive an email saying that you are filling up all the memory of the common temporary folder. It happened to me!
export TMPDIR=$HOME/.local

echo "START ----------------------------"

#load your program if it is installed globally or the modules you used to install your program locally (compilers, etc) 
#Check what is available with module avail
#module load languages/R-3.0.2

# Run program

~/software/BayeScan2.1/binaries/BayeScan2.1_linux64bits AA216.bayescan -output AA216.out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 -pr_odds 20


move the output on server to 
/newhome/aj18951/1a_Aricia_agestis_PopGenomics/Bayescan/Out_perpop

```








Run Bayescan a second time assigning pops by HostPlant rather than geographic populations. 
```


```


Copy these reults to the mac. Check that both runs have converged and plot the outliers: 

PerPop
```
source("plot_R.r")
library(coda)

chain <- read.table("AA216.baye.sel", header=T)
chain <- mcmc(chain, thin=10)

plot(chain)  ##check for convergence
summary(chain)


Iterations = 1:49981
Thinning interval = 10 
Number of chains = 1 
Sample size per chain = 4999 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

           Mean        SD  Naive SE Time-series SE
logL -1.739e+05 2.379e+02 3.365e+00      7.8372071
Fst1  2.719e-01 3.091e-03 4.372e-05      0.0000934

2. Quantiles for each variable:

           2.5%        25%        50%        75%      97.5%
logL -1.744e+05 -1.741e+05 -1.739e+05 -1.737e+05 -1.735e+05
Fst1  2.659e-01  2.698e-01  2.719e-01  2.740e-01  2.778e-01


autocorr.diag(chain) ## check correlation between the chains. Make sure the chains didn't get stuck

              logL        Fst1
Lag 0   1.00000000  1.00000000
Lag 10  0.61856447  0.66385936
Lag 50  0.16025335  0.11518471
Lag 100 0.05900741 -0.00130623
Lag 500 0.05123369  0.03897163

effectiveSize(chain) ##check that this is close to the sample size (here 5000). If there is correlation (chain got stuck) the sample size will be much smaller than the input
    logL      Fst1 
 921.7692 1095.2737 

geweke.diag(chain, frac1=0.1, frac2=0.5)  ##The diagnostic reports the z-scores for each parameter. For example, with α = 0.05, the critical values of z are – 1.96 and +1.96. We reject H0 (equality of means => convergence) if z < -1.96 or z > +1.96.

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

  logL   Fst1 
-1.133 -1.336 


heidel.diag(chain, eps=0.1, pvalue=0.05) ##another test whether the chains have reached stationarity. 

```



## Environmental Association Analyses

### BayEnv2

Input files:

https://bitbucket.org/tguenther/bayenv2_public/src/8e4039f64d61?at=default

Input file with subset of 500 SNPs for co-variance matrix (pop structure)

Input file with all SNPs for the association analysis

ENV input file with normalised environmental parameters. Here I use HostPlant at site. 

NB: population order in the input files should all be the same.

1. Convert to Bayenv

Use pgdspider2.1.0.3 and popfile for pgdspider

I'm using the filtered dataset: only one SNP per locus. This way I can calculate the neutral co-variance in the dataset.


2. Select 500 SNPs for covariance matrix


First convert vcf to bayenv using the thinned vcf file. This is ~1200 unlinked loci

Then use sed to select the first 500 loci in the file. These are called by contig, but have not been sorted using Heliconius, so should represent a random subset of the genome.

```
sed -n 1,1000p AA212.bayenv.thinned.txt > AA212.bayenv.500loci.txt
```

3. Estimate covariance matrix

This runs fairly quickly (~2hours for 132 indivs, 500loci), so I ran it on the mac.

The command needs to be run from the installation folder: /Users/alexjvr/Applications/bayenv2/compiled_on_a_mac
```
cp ~/2018.postdoc/BrownArgus_2018/DataAnalysis_20181116/BayEnv/AA212.bayenv.thinned.txt .

./bayenv2 -i AA212.bayenv.thinned.txt -p 9 -k 100000 r23468 > AA212.500.matrix.out

./bayenv2 -i AA212.bayenv.thinned.txt -p 9 -k 100000 r67328 > AA212.500.matrix2.out
```
Check that the covariance matrix are highly correlated within and between runs.

I will copy 10 matrices randomly from each of the two runs, and visualise their correlation in R. As well as compare this with the pairwise Fst (this can be used to double check the order of the populations in the matrix).

I made new files for a matrix every 10k steps from both runs. ie. 10 matrices per run.
```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/DataAnalysis_20181116/BayEnv

matrix1.10 <- read.table("matrix1.10")
matrix1.10 <- as.matrix(matrix1.10)

matrix2.10 <- read.table("matrix2.10")
matrix2.10 <- as.matrix(matrix2.10)

par(mfrow=c(1,2))
image(matrix1.10)
image(matrix2.10)


cor.test(matrix1.10, matrix2.10)

Pearson's product-moment correlation

data:  matrix1.10 and matrix2.10
t = Inf, df = 79, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 1 1
sample estimates:
cor 
  1 

##plot
par(mfrow=c(1,2))
image(matrix1.10)
image(matrix2.10)
```





### LFMM





## Concatenation of results


