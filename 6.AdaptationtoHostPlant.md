# Adaptation at new sites

1) Adaptation to different host plants?

2) Selection for dispersing individuals? 

3) FlyBase to blast CDS of outlier scaffolds/contigs



## Outlier analyses


### Bayescan

Convert vcf to bayescan using pgdspider. 

Copy file to bluecrystal to run analysis. 

```
pwd
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/Bayescan

scp AA216.bayescan aj18951@bluecrystalp3.acrc.bris.ac.uk:/newhome/aj18951/1a_Aricia_agestis_PopGenomics/Bayescan/
```


I've created a submission script (bayescan.HostPlant.sh) on bluecrystal. To be run from the directory where the input file is located.
```
#!/bin/bash
#PBS -N bayescan1
#PBS -l nodes=1:ppn=1 #1nodes, 1 processor per node
#PBS -l mem=16gb #RAM
#PBS -l walltime=20:00:00 ##20 hours wall time.  
#PBS -j oe  #concatenates error and output files (with prefix job1)
##PBS -t 1-10

#run job in working directory
cd $PBS_O_WORKDIR  #run job in working directory
#cd 1a_Aricia_agestis_PopGenomics/Bayescan

# set a local temporary folder.  Useful if you create a lot of temp files and do not want to receive an email saying that you are filling up all the memory of the common temporary folder. It happened to me!
export TMPDIR=$HOME/.local

echo "START ----------------------------"

#load your program if it is installed globally or the modules you used to install your program locally (compilers, etc) 
#Check what is available with module avail
#module load languages/R-3.0.2

# Run program

~/software/BayeScan2.1/binaries/BayeScan2.1_linux64bits AA251.HostPlant.bayescan -output AA251.HostPlant.out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 -pr_odds 20


move the output on server to 
/newhome/aj18951/1a_Aricia_agestis_PopGenomics/Bayescan/Out_HostPlant

```

This runs for ~




Copy these reults to the mac. Check that both runs have converged and plot the outliers: 

```
source("plot_R.r")
library(coda)

chain <- read.table("AA251.HostPlantPop.baye.sel", header=T)
chain <- mcmc(chain, thin=10)

plot(chain)  ##check for convergence
summary(chain)


Iterations = 1:49981
Thinning interval = 10 
Number of chains = 1 
Sample size per chain = 4999 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

           Mean        SD  Naive SE Time-series SE
logL -2.135e+05 2.937e+02 4.153e+00      1.492e+01
Fst1  7.036e-03 2.608e-04 3.689e-06      1.300e-05
Fst2  2.645e-02 3.369e-04 4.765e-06      7.035e-06

2. Quantiles for each variable:

           2.5%        25%        50%        75%      97.5%
logL -2.141e+05 -2.137e+05 -2.135e+05 -2.133e+05 -2.130e+05
Fst1  6.531e-03  6.859e-03  7.034e-03  7.219e-03  7.535e-03
Fst2  2.580e-02  2.622e-02  2.646e-02  2.668e-02  2.711e-02


autocorr.diag(chain) ## check correlation between the chains. Make sure the chains didn't get stuck

               logL        Fst1        Fst2
Lag 0    1.00000000  1.00000000 1.000000000
Lag 10   0.74665473  0.83642098 0.370840384
Lag 50   0.40182946  0.45318470 0.022659913
Lag 100  0.22318349  0.24408238 0.023834552
Lag 500 -0.07627577 -0.08433466 0.004882332


effectiveSize(chain) ##check that this is close to the sample size (here 5000). If there is correlation (chain got stuck) the sample size will be much smaller than the input
     logL      Fst1      Fst2 
 387.1385  402.5517 2293.8773 

geweke.diag(chain, frac1=0.1, frac2=0.5)  ##The diagnostic reports the z-scores for each parameter. For example, with α = 0.05, the critical values of z are – 1.96 and +1.96. We reject H0 (equality of means => convergence) if z < -1.96 or z > +1.96.

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

    logL     Fst1     Fst2 
 0.25175  0.03646 -0.31611 
 
 

heidel.diag(chain, eps=0.1, pvalue=0.05) ##another test whether the chains have reached stationarity. 

     Stationarity start     p-value
     test         iteration        
logL passed       1         0.873  
Fst1 passed       1         0.944  
Fst2 passed       1         0.487  
                                  
     Halfwidth Mean      Halfwidth
     test                         
logL passed    -2.14e+05 2.93e+01 
Fst1 passed     7.04e-03 2.55e-05 
Fst2 passed     2.65e-02 1.38e-05 
```

All parameters have converged

![alt_txt][bayescanHP.conv]

[bayescanHP.conv]:https://user-images.githubusercontent.com/12142475/54371685-bdbc6500-4671-11e9-84aa-a8e6173aaa3f.png



Plot cut-off for outliers
```
AA.results <- read.table("AA216.HostPlantPop.baye_fst.txt")

pdf("AA.HostPlant.bayescan.results")
par(mfrow=c(1,2))
plot_bayescan(AA.results, FDR=0.05, add_text=T)
plot_bayescan(AA.results, FDR=0.01, add_text=T)
dev.off()


plot_bayescan(AA.results, FDR=0.05, add_text=T)
$outliers
 [1]  2072 10676 11041 11042 11050 12108 16942 17001 21375 21377 21378 21379

$nb_outliers
[1] 12


> plot_bayescan(AA.results, FDR=0.01, add_text=T)
$outliers
[1] 11041 11042 11050 12108 17001

$nb_outliers
[1] 5

```


![alt_txt][HP.bayes.FDR0.05]

[HP.bayes.FDR0.05]:https://user-images.githubusercontent.com/12142475/54372092-8f8b5500-4672-11e9-92be-677cc5fc2cb2.png


![alt_txt][HP.bayes.FDR0.01]

[HP.bayes.FDR0.01]:https://user-images.githubusercontent.com/12142475/54372111-9ade8080-4672-11e9-8b7f-30df74d86053.png




#### Jackknife for Bayescan

To determine if any single population affects the identification of outliers, I'm running bayescan with each of the populations removed. 

I've repeated this analysis with populations identified as new or established (old) under ColHistory. This had to be rerun because FOR was mis-identified as an established site in the initial runs in 2019. 

```
/newhome/aj18951/1a_Aricia_agestis_PopGenomics/Bayescan
```






### BLAST of candidates

Working on the server
```
/newhome/aj18951/1a_Aricia_agestis_PopGenomics/RefGenome
```

I have a list of the contigs/scaffolds where outliers for adaptation to host plant preference were found: 
```
grep ">" outlier.fasta >> outlier.names
sed -i 's/>//g' outlier.names
cat outlier.names
m_scaff_962
m_scaff_5443
contig_41650
m_scaff_4913
m_scaff_5318
m_scaff_5502
m_scaff_5215
m_scaff_4152
m_scaff_4399
m_scaff_6168
m_scaff_486
m_scaff_5063
```

Extract info from gff for each of the outliers: 
```
for f in `cat outlier.names`; do grep $f Aricia_agestis_Red_MESPA.gff >> outlier.bed; done
grep cds outlier.bed >> outlier2.bed
awk -F'\t' '{print $1,"\t",$4,"\t",$5}' outlier2.bed >> outlier3.bed
##sed -i 's/ //g' outlier3.bed ##uncomment if compaint about needing tab delimited file


module load apps/bedtools2
bedtools getfasta -fi Aricia_agestis_Red_MESPA.fasta -bed outlier3.bed -fo out.fasta
```

Submit to [FlyBase]()


### Go Ontology

Gene ontology enrichment in outlier loci

```
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("topGO", version = "3.8")



```



## THE Following analyses were NOT used in final paper ##

PCAdapt

BayEnv

LFMM

### PCAdapt

Initial PCAdapt steps completed in [3.GeneticDiversity&Structure.md](https://github.com/alexjvr1/AriciaAgestis_PopGenMS/blob/master/3.GeneticDiversity%26Structure.md)


Vignette: https://cran.r-project.org/web/packages/pcadapt/vignettes/pcadapt.html

Useful info about interpreting p-value distributions: http://varianceexplained.org/statistics/interpreting-pvalue-histogram/

I did this in R following the vignette quite closely:

I chose K=2 following the scree plot from PCAdapt

```
AA261.bed <- read.pcadapt("plink.bed", type="bed")
x.maf0.05 <- pcadapt(AA261.bed, K=2, min.maf=0.05)   ##calculate z-statistics and transformed values for chi-squared distribution
x.maf0.1 <- pcadapt(AA261.bed, K=2, min.maf=0.01) #calculate for maf 0.1

pdf(file="pcadapt.pvalues_AA261.pdf")
par(mfrow=c(2,1))
hist(x.maf0.05$pvalues,xlab="p-values maf0.05",main=NULL,breaks=50)
hist(x.maf0.1$pvalues,xlab="p-values maf0.01",main=NULL,breaks=50)
dev.off()

```


![alt_txt][maf]

[maf]:https://user-images.githubusercontent.com/12142475/54209447-b2343700-44d5-11e9-82ed-51999125f95c.png



I'll use a MAF cut-off of 1% as this yields more candidate loci, and the p-value distribution is fairly normal. 


use qvalue in R to specify FDR

```
library(qvalue)
alpha <- 0.05
qval <- qvalue(x.maf0.05$pvalues)$qvalues
outliers <- which(qval<alpha)
outliers
snp_pc <- get.pc(x.maf0.05,outliers) ##see PCs associated with the outliers
```

The outliers identified are mostly associated with principal component 2. I.e. they explain the divergence of HOD from the rest of the populations. 


Write qval for both analyses to file. These will be ordered later for comparison between all analyses
```
qval <- qvalue(x.maf0.05$pvalues)$qvalues
write.table(qval, "PCAdapt.qval.maf0.05_AA216", quote=F, row.names=F, col.names=F)

qval <- qvalue(x.maf0.1$pvalues)$qvalues
write.table(qval, "PCAdapt.qval.maf0.01_AA216", quote=F, row.names=F, col.names=F)
```


## Environmental Association Analyses

### BayEnv2

Input files:

https://bitbucket.org/tguenther/bayenv2_public/src/8e4039f64d61?at=default

Input file with subset of 500 SNPs for co-variance matrix (pop structure)

Input file with all SNPs for the association analysis

ENV input file with normalised environmental parameters. Here I use HostPlant at site. 

NB: population order in the input files should all be the same.

1. Convert to Bayenv

Use pgdspider2.1.0.3 and popfile for pgdspider

I'm using the filtered dataset: only one SNP per locus. This way I can calculate the neutral co-variance in the dataset.


2. Select 500 SNPs for covariance matrix


First convert vcf to bayenv using the thinned vcf file. This is ~1200 unlinked loci

```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/BayENV


vcftools --vcf AA261.0.5miss.9popsMerged.vcf --thin 600 --recode --recode-INFO-all --out AA261.thin

VCFtools - v0.1.14
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf AA261.0.5miss.9popsMerged.vcf
	--recode-INFO-all
	--thin 600
	--out AA261.thin
	--recode

After filtering, kept 261 out of 261 Individuals
Outputting VCF file...
After filtering, kept 3819 out of a possible 31381 Sites
Run Time = 6.00 seconds

```

Then use sed to select the first 500 loci in the file. These are called by contig, but have not been sorted using Heliconius, so should represent a random subset of the genome.

```
sed -n 1,1000p AA261.thin.BayEnv.txt > AA261.bayenv.500loci.txt
```

3. Estimate covariance matrix

This runs fairly quickly (~2hours for 132 indivs, 500loci), so I ran it on the mac.

The command needs to be run from the installation folder: /Users/alexjvr/Applications/bayenv2/compiled_on_a_mac
```
cp AA261.bayenv.500loci.txt /Users/alexjvr/Applications/bayenv2/compiled_on_a_mac/

cd /Users/alexjvr/Applications/bayenv2/compiled_on_a_mac/

./bayenv2 -i AA261.bayenv.500loci.txt -p 9 -k 100000 r23468 > AA261.500.matrix.out

./bayenv2 -i AA261.bayenv.500loci.txt -p 9 -k 100000 r67328 > AA261.500.matrix2.out
```
Check that the covariance matrix are highly correlated within and between runs.

I will copy 10 matrices randomly from each of the two runs, and visualise their correlation in R. As well as compare this with the pairwise Fst (this can be used to double check the order of the populations in the matrix).

I made new files for a matrix every 10k steps from both runs. ie. 10 matrices per run.
```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/DataAnalysis_20181116/BayEnv

matrix1.10 <- read.table("matrix1.10")
matrix1.10 <- as.matrix(matrix1.10)

matrix2.10 <- read.table("matrix2.10")
matrix2.10 <- as.matrix(matrix2.10)


matrix1.100 <- read.table("matrix1.100")
matrix1.100 <- as.matrix(matrix1.100)

matrix2.100 <- read.table("matrix2.100")
matrix2.100 <- as.matrix(matrix2.100)


par(mfrow=c(1,2))
image(matrix1.10)
image(matrix2.10)


diss1.10 <- 1-cor(matrix1.10)
diss1.100 <- 1-cor(matrix1.100)
diss2.10 <- 1-cor(matrix2.10)
diss2.100 <- 1-cor(matrix2.100)


##calculate correlation between all individuals and determine whether matrixes are converged within and between runs: 

> cor.test(diss1.10, diss1.100)

	Pearson's product-moment correlation

data:  diss1.10 and diss1.100
t = Inf, df = 79, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 1 1
sample estimates:
cor 
  1 

> diss2.10 <- 1-cor(matrix2.10)
> cor.test(diss2.10, diss2.100)

	Pearson's product-moment correlation

data:  diss2.10 and diss2.100
t = Inf, df = 79, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 1 1
sample estimates:
cor 
  1 

> cor.test(diss1.10, diss2.10)

	Pearson's product-moment correlation

data:  diss1.10 and diss2.10
t = 16.801, df = 79, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.8247593 0.9239490
sample estimates:
      cor 
0.8839262 

> cor.test(diss1.100, diss2.100)

	Pearson's product-moment correlation

data:  diss1.100 and diss2.100
t = 16.801, df = 79, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.8247593 0.9239490
sample estimates:
      cor 
0.8839262 
```


Plot distance trees and visually inspect:
```
par(mfrow=c(2,1))
plot(hclust(as.dist(diss1.10)), main="matrix1.10")
plot(hclust(as.dist(diss2.10)), main="matrix2.10")
par(mfrow=c(2,1))
plot(hclust(as.dist(diss2.100)), main="matrix2.100")
plot(hclust(as.dist(diss1.100)), main="matrix1.100")
```

![alt_txt][hclust]

[hclust]:https://user-images.githubusercontent.com/12142475/54624096-a52ebf00-4a64-11e9-948a-e2989048ec01.png



##### Prepare all the input files

1. convert vcf to bayenv input using pgdspider

2. copy the last covariance matrix out of one of the matrix files and paste in a new file xx.MATRIX

3. Create the xx.ENV file by transposing the variables. ie. populations are columns. I'm not normalising these as they're just binary. 


The analysis is run in the folder with the source code.
```
/Users/alexjvr/Applications/bayenv2/compiled_on_a_mac/AAgestis261

##split the input file into single locus files

split -a 10 -l 2 AA261.0.5miss.9popsMerged.BayENV.txt snp_batch

head *ENV   ##the ENV file format. Populations in alphabetical order as in the vcf file. 

2	1	2	1	1	1	2	1	2
1	1	2	1	2	1	2	1	2



#Start the run

for f in $(ls snp_batch*); do ./bayenv2 -i $f -e AA261.9pops.ENV -m AA261.MATRIX -k 100000 -p 9 -n 2 -r $RANDOM -t -c -X; done
```


I submitted this in a script on bluecrystal: 
```
#!/bin/bash
#PBS -N bayEnv1
#PBS -l nodes=1:ppn=1 #1nodes, 1 processor per node
#PBS -l mem=16gb #RAM
#PBS -l walltime=100:00:00 ##20 hours wall time.  
#PBS -j oe  #concatenates error and output files (with prefix job1)
##PBS -t 1-100

#run job in working directory
cd $PBS_O_WORKDIR  #run job in working directory
#cd 1a_Aricia_agestis_PopGenomics/Bayescan

# set a local temporary folder.  Useful if you create a lot of temp files and do not want to receive an email saying that you are filling up all the memory of the common temporary folder. It happened to me!
export TMPDIR=$HOME/.local

echo "START ----------------------------"

#load your program if it is installed globally or the modules you used to install your program locally (compilers, etc) 
#Check what is available with module avail
#module load languages/R-3.0.2

##set up array

#NAME=$(sed "${PBS_ARRAYID}q;d" snp_batch)

#./bayenv2 -i $NAME -e AA261.9pops.ENV -m AA261.MATRIX -k 100000 -p 9 -n 2 -r $RANDOM -t -c -X

for f in $(ls snp_batch/snp_batch*); do ./bayenv2 -i $f -e AA261.9pops.ENV -m AA261.MATRIX -k 100000 -p 9 -n 2 -r $RANDOM -t -c -X; done
```






### LFMM

See tutorial for command line version: http://membres-timc.imag.fr/Olivier.Francois/lfmm/files/LEA_1.html

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/LFMM/SE132.FINAL

Input files

.env environmental file

1.Genotype file: vcf file of loci genotyped across all populations. 

2. Env file: environmental information for each individual


Env File
```
library(LEA)

env <- read.table("EnvVars_noheaders_AA216", header=F) ###read in the environmental data. One line perindividual. Same order as .vcf file

#Remember that the env variables need to be numeric. Here Var1 = Col history.
#old=1, new=2
#Var2=HostPlant
#Rockrose=1
#Geranium=2

write.env(env, "AA216.env")   ##convert to correct .env format
##Check that this file is numeric! i.e. remove any quote marks using sed in bash or in as.numberic in R. 
##This gives a very uninformative error if the input file is wrong. It says the first SNP is fixed across populations, and then it quits with "Error in obj.lfmm... "
```


Genotype file.
```
Convert .vcf to .lfmm in LEA

library(LEA)
genotype = vcf2lfmm("AA261.0.5miss.9popsMerged.vcf")

	- number of detected individuals:	261
	- number of detected loci:		31381

For SNP info, please check ./AA261.0.5miss.9popsMerged.vcfsnp.

0 line(s) were removed because these are not SNPs.
Please, check ./AA261.0.5miss.9popsMerged.removed file, for more informations.


	- number of detected individuals:	261
	- number of detected loci:		31381
```

Population structure with SNMF
The first step is to evaluate the population structure. SNMF works on the same principles as STRUCTURE, but is much faster.
```
Test K1:10

obj.snmf = snmf(genotype, K=1:10, entropy=T, ploidy=2, project="new")

##The project is saved into : AA216.thin.recode.snmfProject

##To load the project, use:

project = load.snmfProject("AA216.thin.recode.snmfProject")

##To remove the project, use: remove.snmfProject("AA216.thin.recode.snmfProject")

Since the program was called with the entropy option, we can plot the values of the cross-entropy criterion for each K. The value for which the function plateaus or increases is our estimate of K

plot(obj.snmf)
```


![alt_txt][snmf]

[snmf]:https://user-images.githubusercontent.com/12142475/54373447-2b1dc500-4675-11e9-9e76-47ab8b3d49c9.png



The minimum is at 2 (fastStructure also suggested 2). We can visualise the plot
```
barplot(t(Q(obj.snmf, K=2)), col=1:2)

```


![alt_txt][snmf_barplot]

[snmf_barplot]:https://user-images.githubusercontent.com/12142475/54373561-60c2ae00-4675-11e9-9a26-907307e873ae.png

Populations in alphabeticaal order


Run LFMM
```
obj.lfmm = lfmm(genotype, "AA216.env", K=2, rep=5, project="new")  #This was using HostPlant and ColHistory
```
I ran this on the fgcz server. 

```
/srv/kenlab/alexjvr_p1795/AAgestis/LFMM

```





## Concatenation of results



First sort the Aricia agestis gff file according to the Hmel gff file, so that we can assign linkage groups and start positions to each line. 
```

library(data.table)

### Load and reformat Hmel gff for later

Hmel25gff <- read.table("Hmel2.5_LG/Heliconius_melpomene_melpomene_Hmel2.5.gff3",sep="\t")

Hmel25gff_collapsed <- Hmel25gff[Hmel25gff$V3=="gene",]
Hmel25gff_collapsed$ID <- as.character(gsub("ID=", "\\1", Hmel25gff_collapsed$V9))



### Load and reformat Aricia gff
library(data.table)
library(dplyr)
library(ggplot2)

Argff <- read.table("Redundans_loop/Aricia_agestis/Aricia_agestis_Red_MESPA.gff",sep="\t")

Argff_collapsed <- Argff[Argff$V3=="cds",]

Argff_collapsed$ID <- as.character(gsub(".*Target=(HMEL[0-9]*..).*", "\\1", Argff_collapsed$V9))

Argff_collapsed$Scaff <- as.character(gsub(".*_(Hmel[0-9]*o) .*", "\\1", Argff_collapsed$V9))
Argff_collapsed$LG <- as.character(gsub(".*_Hmel2([0-9][0-9])[0-9]*o .*", "\\1", Argff_collapsed$V9))

##remove unassignable Hmel gene hits
Argff_collapsed <- Argff_collapsed[Argff_collapsed$LG != '00',]

Argff_unique_fulldata <- setDT(Argff_collapsed)[, lapply(.SD, paste, collapse = "; "), by = V1]

### Collapse Hmel cds hits into one line for each Aricia scaffold
Argff_unique <- setDT(Argff_collapsed)[, .(count = .N, var = uniqueN(Scaff)), by = V1]
##Make a vector of contigs with conflicting Hmel hits
Argff_unique <- Argff_unique$V1[Argff_unique$var==1]

##remove genes with conflicting hits from full data table
Argff_unique_fulldata <- Argff_unique_fulldata[Argff_unique_fulldata$V1 %in% Argff_unique]
##take first geneID from the cds hits
Argff_unique_fulldata2 <- as.data.frame(Argff_unique_fulldata[, paste0("Agene", 1) := tstrsplit(ID, "; ")[1]])
Assignment_table$LG_assigned <- substr(Assignment_table$LG, start = 1, stop = 2)

new <- Assignment_table %>% mutate(Hmel.pos=Hmel25gff_collapsed$V4[match(Agene1,Hmel25gff_collapsed$ID)]) ##look up the Hmel start position for each add a new column to Assignment table. 

ggplot(data=Assignment_table) + geom_point(aes(x=LG_assigned,y=Hmel.pos))
```

![alt_txt][Hmel.pos]

[Hmel.pos]:https://user-images.githubusercontent.com/12142475/54429390-47bf0900-4718-11e9-9de5-561cb89a28e3.png


get all the locus names. I used the lfmm output: 
```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/LFMM

awk '{print $1, $2, $1"_"$2}' AA261.0.5miss.9popsMerged.vcfsnp  > locus.names

```


Back in R in:
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/AdaptationHostPlant

Create a file with 

1. AAgestis locus names

2. Hmel LG

3. Hmel pos

4. Outlier stats for each of the analyses
```

AA261.locus.stats <- read.table("locus.names_AA261.31381", header=F)
head(AA261.locus.stats)

         V1   V2             V3
1 m_scaff_5 5253 m_scaff_5_5253
2 m_scaff_5 5257 m_scaff_5_5257
3 m_scaff_5 5269 m_scaff_5_5269
4 m_scaff_5 5300 m_scaff_5_5300
5 m_scaff_5 5322 m_scaff_5_5322
6 m_scaff_5 5325 m_scaff_5_5325

##Add bayescan info
bayescan <- read.table("AA216.HostPlantPop.baye_fst.txt", header=T)
AA261.locus.stats$Bayescan <- bayescan$fst

##Add Hmel info to this table
AA261.locus.stats <- AA261.locus.stats %>% mutate(Agene1=Assignment_table$Agene1[match(V1, Assignment_table$V1.AAgestis.scaffold)])
AA261.locus.stats$LG.pos <- paste((as.numeric(AA261.locus.stats$LG_assigned)),(as.numeric(AA261.locus.stats$Hmel.pos)), sep="_")

pdf("Bayescan.ordere.pdf")
ggplot(AA261.locus.stats, aes(x=LG.pos, y=Bayescan)) + geom_point()
dev.off()
```

![alt_txt][bayescan.ordered]

[bayescan.ordered]:https://user-images.githubusercontent.com/12142475/54528772-ca90cf80-4975-11e9-8371-199a282dd134.png


1. Many of the outliers are on the unannotated bit of the genome.

2. There's a big outlier on ChromI (sex chrom). (sex-linked selection?) 

3. There are outliers across the whole genome. 


[Manhattan Plot](https://genome.sph.umich.edu/wiki/Code_Sample:_Generating_Manhattan_Plots_in_R)

```



```

