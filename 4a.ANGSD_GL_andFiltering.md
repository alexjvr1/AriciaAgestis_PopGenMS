# ANGSD GL files and filtering

I'm using ANGSD to call GL for the fastSimCoal analysis. As this is a new pipeline I'm testing I need to test all the different filter parameters to use. 

I'm starting with bam files, so there are already some filters on the mapping quality of the sequences. Prior to that there are some crude filters during the demultiplexing and trimming steps. 

For this analysis I should use an outgroup or ancestral genotype. I could use A. artaxerxes if enough SNPs are called. Otherwise I'll use the A.agestis genome and use the folded SFS. Note that Vitor Sousa and Laurent Excoffier suggest that ANGSDs folded SFS is incorrect. 
Thus I should generate unfolded SFS and fold using Vitor's script on his gitHub page. 




#### NOTE

The provided bam files must be mapped to the provided ref or anc file. 

Also I can't use A. artaxerxes as they are sister species rather than A.agestis derived from A.artaxerxes. 

So.. I will calculate the unfolded SFS using the reference (A. agestis) as the ancestral state. ANGSD folds the SFS per population, but we need to fold the SFS with the same minor allele in each population. The ony way to do this is if we provide a specific allele as the major allele. 


## Prep input files

We need bam files mapped to the appropriate reference (i.e. either the ancestor or a reference genome).

To speed up the analysis I will split the ANGSD run across the genome; i.e. all indivs will be analysed for regions 0-x in ARRAY1, x-x1 in ARRAY2, etc. For this we need to split the genome up into regions. 

Find all the regions (i.e. all chromosomes and contigs) from the reference index file (.fasta.fai): 

*How will this change when we have chromosome level scaffolds?? 
```
awk '{print $1}' ../RefGenome/*.fasta.fai >> regions

cat regions |wc -l
>93568

##We can run 100 jobs per ARRAY, so split the regions file into 100 ~equal lines. This runs faster than the previous approach where I made sure each regions file was the same size with a max size per regions file. 

split -l 1000 regions regions
```



## Filters

### Input bam files

-b[filelist]

-remove_bads 1 : remove reads with 255 flag (not primary, failure and duplicate reads) (1=default)

-uniqueOnly 1 : remove reads with multiple best hits 

-minMapQ 20 : PHRED 20. This should already be in place during the mapping. 

-minQ 20 : PHRED 20 for individual base score. 

-only_proper_pairs 1 : include only properly paired reads (default)

-trim 0 : We're not trimming any data

-baq 1 : estimate base alignment quality using samtools method. 

### Filters for bam files

-minMaf 0.05 : set minimum MAF to 5% to exclude spurious data. 

### Allele frequency estimation

-doMajorMinor 4 : Force Major allele based on reference. The minor allele is then inferred using doMajorMinor 1. This option needs to be used when calculating SFS for multiple populations as ANGSD otherwise determines a minor allele within each population. I.e. this may not be the same across all the populations. 

-ref [..fasta] : For doMM 4 above we need to specify a reference genome. 

-doMaf 1 : calculate minor allele frequency 

-SNP_pval [float]: Only work with SNPs with a p-value above [float]



### Genotype likelihood

-GL 1 : I will estimate genotype likelihoods using the SAMtools model

-minInd 18 : I will remove loci where less than 18 individuals have been genotyped. There are 19-45 indivs per group (HOD,FOR, South, New). So this number seems quite high, but this is to be more certain of the 5% MAF. 

-setMinDepth  : Discard site if total depth (across all indivs) is below [int]. Use -doCounts to determine the distribution of depths

-setMaxDepth : Discard site if total depth (across all indivs) is above [int]

-setMinDepthInd 2 : Minimum depth for a locus for an individual. This is only applicable for analyses using counts (-doCounts obligatory)

-setMaxDepthInd [int]: I'll use a max of meanDP + 2xSD of depth. 

-doCounts 1 : Count of the nucleotide bases per individual

-dumpCounts 2 : write a file with all the allele counts per position per individual

-rmTriallelic 1 : include only biallelic loci

-checkBamHeaders 1 : check that the bam headers are compatable for all files. 


I'm running [this](https://github.com/alexjvr1/AriciaAgestis_PopGenMS/blob/master/angsd.sfs.FOR.allFilt.sh) script initially to determine the depth count while including all of the other filters. 


## Depth estimate

First determine the distribution of depth across all sites/indivs to set the min- and maxDepthInd. 

The counts per indiv per site (--dumpCounts 2) are written to ..counts.gz.

```
#First concatenate all these together: 

ls *counts.gz >> counts.list
zcat $(cat counts.list) >> BA.FOR.counts


##Sum all columns = total depth per indiv. This needs to be divided by the number of loci (659141)

awk '{for(i=1;i<=NF;i++)a[i]+=$i;print $0} END{l="SUM";i=1;while(i in a){l=l" "a[i];i++};print l}' BA.FOR.counts > BA.FOR.countSUMS

awk '{ for(i=1; i<=NF;i++) j+=$i; print j; j=0 }' BA.FOR.counts > rowsums 

##Read into R and write a pdf
rowsums <- read.table("rowsums", header=F)
pdf("rowsums.pdf")
hist(rowsums$V1)
dev.off()

##look at the data
summary(rowsums$V1)
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      0    4507    6713    8966   10400  513300 
      
quantile(rowsums$V1, c(0.01, 0.1, 0.5, 0.75, 0.8, 0.9, 0.95, 0.97, 0.99))
     1%     10%     50%     75%     80%     90%     95%     97%     99% 
 1538.0  3202.0  6713.0 10399.0 11632.0 15489.0 20235.0 25165.0 41729.6 
```
The mean and the median are not that different. And 75% of the data have a depth <10400X. 

Calculate the mean + 2xSD of the depth
```
sd(rowsums$V1)
[1] 10854.47

> (2*10854.47)+8966 
[1] 30674.94
```

### Select filters

Test the effect of different filters: 


The end of each log file counts how many sites were processed and how many were kept after filtering
```
>tail -n 20 BA.sfs.SNPP.FINAL.FOR.ARRAY.o9270765-1
	-> Done waiting for threads
	-> Output filenames:
		->"test_pval_nomaf/BA.FOR.regionsaa.arg"
		->"test_pval_nomaf/BA.FOR.regionsaa.pos.gz"
		->"test_pval_nomaf/BA.FOR.regionsaa.counts.gz"
		->"test_pval_nomaf/BA.FOR.regionsaa.mafs.gz"
		->"test_pval_nomaf/BA.FOR.regionsaa.geno.gz"
		->"test_pval_nomaf/BA.FOR.regionsaa.saf.gz"
		->"test_pval_nomaf/BA.FOR.regionsaa.saf.pos.gz"
		->"test_pval_nomaf/BA.FOR.regionsaa.saf.idx"
	-> Mon Apr  6 21:07:22 2020
	-> Arguments and parameters for all analysis are located in .arg file
	-> Total number of sites analyzed: 256447
	-> Number of sites retained after filtering: 668 
	[ALL done] cpu-time used =  43.12 sec
	[ALL done] walltime used =  44.00 sec

real	0m43.772s
user	0m42.461s
sys	0m0.694s

```

We can compare the total sites retained for the different filter sets: 
```
grep "retained" BA.sfs.SNPP.FINAL.FOR.ARRAY.o9270765-* | awk -F "filtering:" '{s+=$2} END {print s}'
```

#### *Tested:* 

###### 1. 85% genotyping rate, SNP_pval 0.001, no MAF filter

> 15908 sites retained of total 7643199

###### 2. 85% genotyping rate, MAF 0.05

> 11416 sites retained of total 7643199

###### 3. 85% genotying rate, MAF 0.00

> 772288 sites retained of total 7643199



I'm using filter set 1. No MAF cut-off, but a cut-off for the SNP p-value, using [this](https://github.com/alexjvr1/AriciaAgestis_PopGenMS/blob/master/angsd_filters_minmaxDP_SNPPval_FOR_ARRAY.sh) script

### Concat all together

After including the min and max depth filters, I need to concat the entire dataset together. 



### Filter for 1 SNP per RADtag

SFS is based on unlinked SNPs so  I need to filter for 1 SNP per RAD tag. 
