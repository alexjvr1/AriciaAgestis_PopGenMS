# UK colonisation history

Where were the new populations colonised from? We will use fastSimCoal to estimate the most likely colonisation history. 

We do not have the ancestral state, so I have to generate the [folded SFS](https://www.mun.ca/biology/scarr/SFS_&_FSFS.html) for each of the populations or groups of populations. That is, if all derived alleles are identified as compared with Indiv 1 (a random indiv in the population), then the frequency of the derived alleles are combined for classes i and (n-i), where i is the number of indivs in the population. 

An [example paper](https://nph.onlinelibrary.wiley.com/doi/full/10.1111/nph.14951#nph14951-fig-0002) from the Sork lab. 

And [another](https://onlinelibrary.wiley.com/doi/full/10.1111/mec.15132?casa_token=iTjBMCiloY4AAAAA%3AY2wNHk41rqTXJXb077LRnuAsWMoe0LGFBuPcFpLA0qLJ3gCpHeGePPmHqnMQc2C_LVXe7nTOcfbNcE8) from the Laurila group 


## Filtering

1. The dataset is filtered to remove any loci under selection

2. Populations are downsampled to exclude all missing data using a python script available from the paper. 

3. To exclude linked loci only one variant is included per RADtag. 

This was done on my computer
```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/FastSimCoal

vcftools --vcf AA261.0.5miss.9popsMerged.vcf 

VCFtools - v0.1.14
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf AA261.0.5miss.9popsMerged.vcf

After filtering, kept 261 out of 261 Individuals
After filtering, kept 31381 out of a possible 31381 Sites
```

First remove all loci under selection. For this I need to identify a list of outliers for 1) host plant preference, and 2) colonisation history. Three of these overlaps. I'll remove the entire scaffold or contig that contains outlier loci. These vcf files have already been generated for fineRADstructure: 
```
## ColHistory 

--vcf /Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/fineRADstructure/AA261.ColHistOutlierScaffolds.recode.vcf 

VCFtools - v0.1.14
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf /Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/fineRADstructure/AA261.ColHistOutlierScaffolds.recode.vcf

After filtering, kept 261 out of 261 Individuals
After filtering, kept 169 out of a possible 169 Sites
Run Time = 1.00 seconds


Col hist
	--chr contig_16912
	--chr contig_1784
	--chr contig_18281
	--chr contig_1883
	--chr contig_19343	** in both
	--chr contig_19564
	--chr contig_19566
	--chr contig_24918
	--chr contig_3360
	--chr contig_3838	** in both  
	--chr contig_5345	** in both
	--chr contig_59667


## HostPlant

vcftools --vcf /Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/fineRADstructure/AA261.HostPlantOutlierScaffolds.recode.vcf 

VCFtools - v0.1.14
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf /Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/fineRADstructure/AA261.HostPlantOutlierScaffolds.recode.vcf

After filtering, kept 261 out of 261 Individuals
After filtering, kept 109 out of a possible 109 Sites
Run Time = 0.00 seconds


HostPlant
	--chr contig_11951
	--chr contig_17378
	--chr contig_19343	** in both
	--chr contig_2518
	--chr contig_3838	** in both
	--chr contig_5345	** in both
	--chr contig_5407
	--chr m_scaff_962
	
#Remove these from main vcf file

vcftools --gzvcf AA261.0.5miss.9popsMerged.vcf.gz --not-chr m_scaff_962 --not-chr contig_5407 --not-chr contig_5345 --not-chr contig_3838 --not-chr contig_2518 --not-chr contig_19343 --not-chr contig_17378 --not-chr contig_11951 --not-chr contig_16912 --not-chr contig_1784 --not-chr contig_18281 --not-chr contig_1883 --not-chr contig_19564 --not-chr contig_19566 --not-chr contig_24918 --not-chr contig_3360 --not-chr contig_59667 --recode --recode-INFO-all --out AA261.0.5miss.9popsMerged.Neutral


...After filtering, kept 31158 out of a possible 31381 Sites
```

Downsample to one SNP per locus to minimise linkage disequilibrium in the dataset. RAD tags are max 600bp, but linkage blocks could be larger. I'm using --thin to downsample to one SNP per 10kb. 
```
vcftools --vcf AA261.0.5miss.9popsMerged.Neutral.recode.vcf --thin 10000 --recode --recode-INFO-all --out AA261.Neutral.thin

After filtering, kept 261 out of 261 Individuals
Outputting VCF file...
After filtering, kept 3486 out of a possible 31158 Sites
Run Time = 4.00 seconds


##If thinning for 600bp (i.e. size of RAD tags) we get 3798 loci. 
```

## Input files

I'm using easySFS to generate the SFS first for the BaseModel and then for the full model. 

I'm using a modification of Vitor's scripts to call multiple FSC runs and then to summarise the results. 

Script to run FSC: [FSC.BaseModel.ARRAY.sh](https://github.com/alexjvr1/AriciaAgestis_PopGenMS/blob/master/4e_FSC.BaseModel.ARRAY.sh)
Submit by inputting options in the command line

```
qsub FSC.BaseModel.ARRAY.sh -F "BA BaseModel BaseModel_ MSFS.obs 100"

poptag=$1 #Analysis name
tplEstTag=$2  #Prefix of est file
obsSFSfile=$3  #MSFS file prefix
obsFileEnding=$4  #MSFS (or other SFS) file suffix
numRuns=$5  #number of array jobs. But see below. I create the numRunsSeq2 file beforehand 

#Define Array names
#seq $baseRuns $numRuns >> numRunsSeq
#sed 's/^/Run/g' numRunsSeq >> numRunsSeq2
NAME=$(sed "${PBS_ARRAYID}q;d" numRunsSeq2)
```

File Organisation: 

The model directory should contain the SFS.obs, the .tpl and .est files, as well as the numRunsSeq2 file. All scripts (.sh) are in the same folder as the inputs: 

```
>ls BaseModel_Model2

BA-BaseModel_MSFS.obs         
BaseModel.est                 
BaseModel_jointMAFpop1_0.obs  
BaseModel_jointMAFpop2_0.obs  
BaseModel_jointMAFpop2_1.obs  
BaseModel.tpl
FSC.BaseModel.ARRAY.sh  
FSC.CollectResults.sh   
numRunsSeq2
```


After this is done we need to move the restults up one directory. All results are written in Runxx (1-100), and then in a $ModelName folder. Move everything up one directory to be in the Run* folder. 
```
for dir in *; do mv "$dir"/BA-BaseModel/* "$dir"/; done
```

Check that this has worked. The bestlhoods file should be in directly in the Runx directory: 
```
>ls Run100/

BA-BaseModel             BA-BaseModel.bestlhoods    BA-BaseModel.est       BA-BaseModel_MSFS.obs  BA-BaseModel.par  BA-BaseModel.tpl
BA-BaseModel_1.simparam  BA-BaseModel.brent_lhoods  BA-BaseModel_maxL.par  BA-BaseModel_MSFS.txt  BA-BaseModel.pv   seed.txt
```

And summarise all the results to create the xx_ALL.param file
Script to summarise all the results of a particular model: [FSC.CollectResults.sh](https://github.com/alexjvr1/AriciaAgestis_PopGenMS/blob/master/4f_FSC.CollectResults.sh)


## Analyse outputs

I'm analysing the outputs using [Vitor's scripts](https://github.com/vsousa/EG_cE3c/blob/c3a8871220cc0fef31f1bc0e347e3d295f2d7d93/CustomScripts/Fastsimcoal_ProcessOutput/Scripts_AnalyseFsc/utilFscOutput.r) modified for the Array job outputs. 

My versions can be found here: 

[AnalyseFscResults.r]()

[ParFileInterpreter_VS.r]()

[utilFscOutput.r]()


These need to be run on the mac. 

R version 3.5.0

1. Copy all the outputs to the mac: 
```
>ls /Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/FastSimCoal/2020_AnalyseOutputs

AnalyseFscResults.r				
ParFileInterpreter_VS.r
utilFscOutput.r

##make a directory for the model to be analysed and copy all outputs from bluecrystal: 

mkdir BA-BaseModel_Model2 
cd BA-BaseModel_Model2

scp -r bluecp3:/newhome/aj18951/1a_Aricia_agestis_PopGenomics/FastSimCoal/BaseModel_Model2/*obs .
scp -r bluecp3:/newhome/aj18951/1a_Aricia_agestis_PopGenomics/FastSimCoal/BaseModel_Model2/Run* .
scp -r bluecp3:/newhome/aj18951/1a_Aricia_agestis_PopGenomics/FastSimCoal/BaseModel_Model2/*_ALL.param .

## rename the two files: 
mv BA-BaseModel_MSFS.obs BaseModel_MSFS.obs
mv BA-BaseModel_ALL.param BA-BaseModel_Model2_ALL.param

cd ..
```

2. Edit the Settings section in AnalyseFscResults.r:
```
####################
## Settings       ##
####################

settings <- list()
# population tag
settings$poptag <- "BA"
# model tag
settings$modeltag <- "BaseModel_Model1"
#filenames added by AJvR
settings$filename <- "BA-BaseModel"
# population names according to order in Obs SFS
settings$pop.names <- c("SOUTH", "FOR", "HOD")
# path to folder with results
# here if your results are in a server you could use
# username@server.address:/folderInServer
settings$pathtofolder <- paste(settings$poptag, "-", settings$modeltag, sep="")
# observed SFS file name and path used to get maximum likelihood estimates
# this can contain linked SNPs
settings$obsfilename <- paste(settings$poptag, "-", settings$modeltag, "/BaseModel_MSFS.obs", sep="")
# observed SFS with only independent SNPs
settings$obsfilename_unlinkedSNPs <- paste(settings$poptag, "-", settings$modeltag, "/BaseModel_MSFS.obs", sep="")
# need an option for multi-SFS
settings$multiSFS <- TRUE
# -C option with minimum SFS counts. All entries with less than -C are pooled together
settings$minentry <- 1
```

3. Run the script
```
Rscript AnalyseFscResults.r
```

### Results

BaseModel 1 vs 2

Model1
```
In dir.create(folderName) : 'BA-BaseModel_Model1' already exists
[1] TRUE
      ANCSIZE        NSOUTH          NHOD          NFOR         TDIV1 
 7.940500e+04  1.020980e+05  2.404500e+04  1.286300e+04  5.310000e+02 
     TPLUSDIV         MIG01         MIG10         MIG02         MIG20 
 1.750000e+02  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 
        MIG12         MIG21       MUTRATE        RESIZE         TDIV2 
 0.000000e+00  0.000000e+00  4.495430e-09  7.777332e-01  7.060000e+02 
  MaxEstLhood   MaxObsLhood 
-4.498638e+03 -4.470925e+03 
cp: ./BA-BaseModel_Model1/Run10 and BA-BaseModel_Model1/Run10 are identical (not copied).
Read 4 items
Read 41412 items
Read 41412 items
[1] "Sum of entries of expected SFS==0 is  29576"
[1] "Sum of entries of observed SFS==0 is  20707"
null device 
          1 
[1] "./BA-BaseModel_Model1/Run10/BA-BaseModel_maxL"
Read 30 items
Loading required package: shape
null device 
          1 
Read 4 items
Read 41412 items
Read 41412 items
$AIC
[1] 20747.78

$loglhood
[1] -4499.244
```


Model2
```
In dir.create(folderName) : 'BA-BaseModel_Model2' already exists
[1] TRUE
      ANCSIZE        NSOUTH          NHOD          NFOR         TDIV1 
 2.033599e+06  5.901520e+05  1.325800e+05  8.856200e+04  4.761000e+03 
     TPLUSDIV         MIG01         MIG10         MIG02         MIG20 
 4.300000e+01  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 
        MIG12         MIG21       MUTRATE        RESIZE         TDIV2 
 0.000000e+00  0.000000e+00  3.993450e-09  3.445890e+00  4.804000e+03 
  MaxEstLhood   MaxObsLhood 
-4.509396e+03 -4.470925e+03 
cp: ./BA-BaseModel_Model2/Run30 and BA-BaseModel_Model2/Run30 are identical (not copied).
Read 4 items
Read 41412 items
Read 41412 items
[1] "Sum of entries of expected SFS==0 is  29783"
[1] "Sum of entries of observed SFS==0 is  20707"
null device 
          1 
[1] "./BA-BaseModel_Model2/Run30/BA-BaseModel_maxL"
Read 30 items
Loading required package: shape
null device 
          1 
Read 4 items
Read 41412 items
Read 41412 items
$AIC
[1] 20797.31

$loglhood
[1] -4509.998
```


################
# DID NOT WORK
################ 
## Input files

Generating the input files for fastSimCoal2

### 1) SFS

I'm using [this](https://github.com/shenglin-liu/vcf2sfs) R script to generate a folded SFS for FOR (called North), HOD, South, and New populations. 

I need a vcf file with a popmap file: 
```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/FastSimCoal

#write all the indiv names to file
bcftools query -l AA261.Neutral.thin.recode.vcf > popmap.AA261 
#write all the popnames to file
awk -F'_' '{print $0}' popmap.AA261 > popnames
#add together
print popmap.AA261 popnames > popmap.AA261.2
rm popmap.AA261 && mv popmap.AA261.2 popmap.AA261
head popmap.AA261 
BAR_10_2013	BAR
BAR_11_2014	BAR
BAR_12_2013	BAR
BAR_13_2014	BAR
```

And download the R script for the above link. The function vcf2sfs.r is stored in the FastSimCoal folder where all these analyses are being run.
Open R: 
```
source("vcf2sfs.r")
mygt <- vcf2gt("AA261.Neutral.thin.recode.vcf", "popmap.AA261")   ##read in the vcf file and popmap files

viewMissing(mygt) ##look at the distribution of missing data
```

#### Distribution of missing data in the dataset

![alt_txt][Missing.data]

[Missing.data]:https://user-images.githubusercontent.com/12142475/76212667-e3d30180-6200-11ea-9427-de8c11ee6a36.png

Filter missing data and subset the populations
```
mygt.filtered <- filter.gt(mygt, filter.indi=NA, filter.snp=T) ##if I filter individuals with missing data all data are removed! This current filter leaves me with only 130 SNPs! This is too few for fastSimCoal. I need at least 2000 loci, although I can remove more individuals. 
Splitting the dataset up into the different groups slightly increases the number of fully genotyped SNPs, but this is still well under 1000 for all groups. 
```
We need several 1000 loci for fastSimCoal to work effectively. See Laurent Excoffier's comment [here](https://groups.google.com/forum/#!topic/fastsimcoal/E_GdjQCGPtU). The current dataset doesn't lend itself to this analysis. One option is to downsample the number of individuals in the dataset. A second option is to partition the dataset into the different groups and then impute the missing data within each dataset. Laurent Excoffier [doesn't like this approach](https://groups.google.com/forum/#!topic/fastsimcoal/czVzOGrPEb8).

Strategy: 

I will remove individuals with the most missing data and downsample to the same number of samples per group. Then I will impute (or should I impute first? But then the groups with larger sample sizes will have a better imputation?). 

The question is how does BEAGLE 1) phase haplotypes and 2) Impute missing genotypes when there is population structure in the dataset. Is it better to split the dataset into the geographic regions first? 

Compromise: 

Remove all indivs with > x% missing data (a low %). Then impute the missing data. The papers I've seen all downsample the geographic regions to include only indivs with a high genotyping rate. In some cases this is as low as [4 indivs!](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6128945/), although all MSs have a larger dataset than us (min 5668 loci). 

We start off with 31k loci, but thin down to ~3000 to reduce linkage in the dataset. 

What % missingness will we allow? There's the usual trade-off between missingness and the number of individuals that drop out
```
vcftools --vcf AA261.Neutral.thin.recode.vcf --missing-indv

AJvR:FastSimCoal alexjvr$ awk '$5>0.06 {print $1}' out.imiss |wc -l
     140
AJvR:FastSimCoal alexjvr$ awk '$5>0.07 {print $1}' out.imiss |wc -l
      89
AJvR:FastSimCoal alexjvr$ awk '$5>0.08 {print $1}' out.imiss |wc -l
      51
AJvR:FastSimCoal alexjvr$ awk '$5>0.09 {print $1}' out.imiss |wc -l
      44
AJvR:FastSimCoal alexjvr$ awk '$5>0.1 {print $1}' out.imiss |wc -l
      39
```
There's a big difference in the number of indivs with max 6% vs 7% missingness. If I reduce down to 6% the North group (HOD) contains only 3 indivs. I think this is too few. So I will use a cut-off of max 7% missingness. And impute the 7% missing data using BEAGLE. 

```
vcftools --vcf AA261.Neutral.thin.recode.vcf --missing-indv

awk '$5>0.07 {print $1}' out.imiss > indivstoremove

vcftools --vcf AA261.Neutral.thin.recode.vcf --remove indivstoremove --recode --recode-INFO-all --out AA261.Neutral.thin.maxmissing0.07percent

VCFtools - 0.1.17
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf AA261.Neutral.thin.recode.vcf
	--remove indivstoremove
	--recode-INFO-all
	--out AA261.Neutral.thin.maxmissing0.07percent
	--recode

Warning: Expected at least 2 parts in INFO entry: ID=PV4,Number=4,Type=Float,Description="P-values for strand bias, baseQ bias, mapQ bias and tail distance bias">
Warning: Expected at least 2 parts in INFO entry: ID=PV4,Number=4,Type=Float,Description="P-values for strand bias, baseQ bias, mapQ bias and tail distance bias">
Warning: Expected at least 2 parts in INFO entry: ID=DP4,Number=4,Type=Integer,Description="Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases">
Warning: Expected at least 2 parts in INFO entry: ID=DP4,Number=4,Type=Integer,Description="Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases">
Excluding individuals in 'exclude' list
After filtering, kept 173 out of 261 Individuals
Outputting VCF file...
After filtering, kept 3486 out of a possible 3486 Sites
Run Time = 3.00 seconds

```

##### Phase and impute using Beagle 5.1


```

```


##### Calculate the folded SFS

######### Tests

1) Test the coalescent model with just a single model first: Divergence between South and HOD. 

Split the vcf file to include only South and HOD. Create the poplist

Then in R: 
```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/FastSimCoal/test/Test1_HOD.South

source("../vcf2sfs.r")

mygt <- vcf2gt("South.HOD.Test.recode.vcf", "names")  ##read in

##calculate SFS & fold
mysfs <- gt2sfs.raw(mygt, c("South", "HOD"))
fold.mysfs <- fold.sfs(mysfs)

plot.sfs(fold.mysfs)

write.2D.fsc(fold.mysfs, "South.HOD.sfs")  ##This script can only write 1D or 2D SFS. So I'll need a different script for the 3pop files. Also later there will be 4-5 demes! 
```
![alt_txt][fold.mysfs.Test1]

[fold.mysfs.Test1]:https://user-images.githubusercontent.com/12142475/77167011-1e645600-6aad-11ea-9ba9-868f092f246b.png







2) Test a three deme model 
First we're estimating the base divergence model between north (HOD, FOR) and South. I'll split each of the vcffiles before reading into R. (This isn't necessary - I only realised later. So for the full dataset I'll just use the single vcf file). 

Then in R: 
```
/Users/alexjvr/2018.postdoc/BrownArgus_2018/201902_DataAnalysis/FastSimCoal/test

source("vcf2sfs.r")  #script

##read in files
southgt <- vcf2gt("South.nomissing.test.recode.vcf", "South.names")
southgt <- vcf2gt("South.nomissing.test.recode.vcf", "South.names.new") ##if all south indivs are seen as a single population

HODgt <- vcf2gt("HOD.nomissing.test.recode.vcf", "HOD.names")
FORgt <- vcf2gt("FOR.nomissing.test.recode.vcf", "FOR.names")


##calculate SFS
mysfs.south <- gt2sfs.raw(southgt,c("BCH","LYD","SWD"))
mysfs.south.new <- gt2sfs.raw(southgt, "South") ##single pop
mysfs.FOR <- gt2sfs.raw(FORgt,"FOR")
mysfs.HOD <- gt2sfs.raw(HODgt,"HOD")



##fold SFS
foldsfs.south <- fold.sfs(mysfs.south)
foldsfs.south.new <- fold.sfs(mysfs.south.new)
foldsfs.HOD <- fold.sfs(mysfs.HOD)
foldsfs.FOR <- fold.sfs(mysfs.FOR)


##plot SFS (can't plot more than 2D

plot.sfs(foldsfs.HOD)
plot.sfs(foldsfs.FOR)

##write files for fastSimCoal

write.1D.fsc(foldsfs.HOD, "foldsfs.HOD")
write.1D.fsc(foldsfs.FOR, "foldsfs.FOR")
write.2D.fsc(foldsfs.south, "foldsfs.south")
write.1D.fsc(foldsfs.south.new, "foldsfs.south.new")
```

![alt_txt][FOR]

[FOR]:https://user-images.githubusercontent.com/12142475/76324228-9d9fa000-62dd-11ea-9527-ce9edf75bf1c.png

![alt_txt][HOD]

[HOD]:https://user-images.githubusercontent.com/12142475/76324232-9e383680-62dd-11ea-9ee0-ef89e9f23909.png


### 2) Priors

I have several parameters that I have to specify priors for. 

I'll use a point value for mutation rate: 

mutation rate - There is only one direct estimate for Lepidoptera (in H. melpomone) [(Keightley et al. 2014b)](https://academic.oup.com/mbe/article/32/1/239/2925597). This is 2.9 × 10−9 (with an upper 95% limit of 5.5 × 10−9). This is similar to in Drosophila: 2.8 × 10−9 per base per haploid genome per generation (with an upper 95% limit of 6.1 × 10−9) [(Keightley et al. 2014a)](https://www.genetics.org/content/196/1/313.short)

nucleotide diversity - estimated [before](https://github.com/alexjvr1/AriciaAgestis_PopGenMS/blob/master/3.GeneticDiversity%26Structure.md)

generation time - Brown argus is bivoltine in the UK, so gen time is 0.5 years. 

Ne = nucleotide diversity / 4xmutation rate = 0.001/(4x (2.9x10^-9)) = 86,206.


### 3) Population input files

For generation of input files see [here](https://speciationgenomics.github.io/fastsimcoal2/) for help. 

The input files need to be named in a consistent way for fastSimCoal2

MAF (or folded SFS)
```
$PREFIX_MAFpop0.obs

mv foldsfs.FOR FOR_MAFpop0.obs
mv foldsfs.HOD HOD_MAFpop0.obs
mv foldsfs.south.new South_MAFpop0.obs

##for multiple populations we need MSFS.obs suffix. And in the fsc2 model specify -multiSFS flag
mv foldsfs.south South_MSFS.obs
```

The population number should follow the tree i.e. South = pop0, HOD = pop1, FOR = pop2. Do I need a multi-dimensional SFS for South? This is what I've created... So, I'll name this accordingly. South_MSFS.obs. Not sure how this will work without pop0 in the title. 
I've created a single MAF file for South where I group all the populations together. This creates a 1D MAF SFS. 



## Scenarios

We're testing three main scenarios. 

1. Col from the south northwards

2. Col from the east

3. Col from the north. 

We'll also include colonisation from all three with mixture between them. 

The scenarios are complicated because we don't know the relationship between all the old populations. So I will run an initial analysis to ascertain the most likely colonisation history of the old populations first. 

```


```


##### ***It is really important to add in a "//Migration matrix x" line if there are any migration matrices. Otherwise the historical events don't get read in properly. 

This is the test model with 2 demes only. 
```
//Number of population samples (demes)
2
//Population effective sizes (number of genes)
NSOUTH
NHOD
//Sample sizes
69
21
//Growth rates	: negative growth implies population expansion
0
//Number of migration matrices : 0 implies no migration between demes
1
//Migration matrix 0
0 MIG10
MIG01 0
//historical event: time, source, sink, migrants, new size, new growth rate, migr. matrix 
1 historical event
TDIV1 0 1 1 RESIZE 0 0
//Number of independent loci [chromosome] 
1 0
//Per chromosome: Number of linkage blocks
1
//per Block: data type, num loci, rec. rate and mut rate + optional parameters
FREQ 1 0 MUTRATE
```


## Folded SFS

Input data needs to be filtered, and the the (joint) folded SFS should be calculated for each of the groups of populations. 

1. North old

2. South old

3. HOD

4. New


### Data filtering: 


```


```


### Split the data into four groups of interest - only if necessary 

Popnames
```
#Extract all sample names
bcftools query -l AA261.0.5miss.9popsMerged.Neutral.recode.vcf

#create files for each population subset
head HOD.popnames 
HOD_10_2013
HOD_1_2013
HOD_12_2014
HOD_14_2013
HOD_15_2014_all
HOD_16_2013
HOD_16_2014


head South.old.popnames 
BCH_10_2013
BCH_1_2014
BCH_12_2013
BCH_12_2014
BCH_14_2014
BCH_15_2013
BCH_15_2014
BCH_16_2013
BCH_19_2013
BCH_19_2014

North.Old.popnames 
FOR_11_2014
FOR_12_2014
FOR_13_2014
FOR_14_2014
FOR_15_2014
FOR_17_2014
FOR_19_2014
FOR_20_2014


```


```
bcftools view -S HOD.popnames AA261.Neutral.thin.recode.vcf > HOD.Neutral.vcf
...After filtering, kept 30 out of 30 Individuals
...After filtering, kept 3486 out of a possible 3486 Sites


bcftools view -S South.old.popnames AA261.Neutral.thin.recode.vcf > South.Neutral.vcf
...After filtering, kept 98 out of 98 Individuals
...After filtering, kept 3486 out of a possible 3486 Sites


bcftools view -S North.Old.popnames AA261.Neutral.thin.recode.vcf > North.Neutral.vcf
...After filtering, kept 20 out of 20 Individuals
...After filtering, kept 3486 out of a possible 3486 Sites


```


# FINAL model with 3 pops (excluding FOR)



```
ls /newhome/aj18951/1a_Aricia_agestis_PopGenomics/FastSimCoal/FINALmodel_Model1

FSC.FULLModel.ARRAY.sh  
FullModel_jointMAFpop2_0.obs  
FullModel_jointMAFpop3_2.obs  
NEW_MAFpop0.obs
FullModel_jointMAFpop2_1.obs  
BA-FullModel_MSFS.obs            
numRunsSeq2      
Run4
FOR_MAFpop0.obs  
FullModel.est  
FullModel_jointMAFpop3_0.obs  
FullModel.tpl             
SOUTH_MAFpop0.obs
FSC.CollectResults.sh  
FullModel_jointMAFpop1_0.obs     
FullModel_jointMAFpop3_1.obs  
HOD_MAFpop0.obs


qsub FSC.FULLModel.ARRAY.sh -F "BA FullModel FullModel_ MSFS.obs 100"

```


